{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0LoIVEn7rKG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Banking.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KVMMDGOa7839"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "aC68Nhzm97S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "dyWRKLz9AnkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above information, we notice that **Loan_Amount_Requested** is in incorrect type, I will be replcing comma to dot in that column and converting to float64 type"
      ],
      "metadata": {
        "id": "H3ZIlbmxZk7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " df['Loan_Amount_Requested']= df['Loan_Amount_Requested'].str.replace(',','.')"
      ],
      "metadata": {
        "id": "4ERpAJy9ZXgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Loan_Amount_Requested'] = df['Loan_Amount_Requested'].astype(np.float64)"
      ],
      "metadata": {
        "id": "t0pH-rKBUDEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we use value_counts() to display how many instances are present in the\n",
        "# categorical feature/class variable\n",
        "df['Interest_Rate'].value_counts()"
      ],
      "metadata": {
        "id": "v4uTs95b9YNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing Data**\n",
        "\n",
        "Preprocessing data includes handling missing values and outliers, applying feature coding techniques if needed, scale & standardize features.\n",
        "\n"
      ],
      "metadata": {
        "id": "8T2cGpc49om3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for Missing values**"
      ],
      "metadata": {
        "id": "AWxRYuJn9wcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# isnull() method can be used to check each cell in the dataset\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ik1jwDlQ9v9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Outliers**\n",
        "\n",
        "We check for outliers only in the features that contain numerical values."
      ],
      "metadata": {
        "id": "DIG1wu21Aasr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "KFfJERMVBNo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Checking the **'Annual_Income'** feature for outliers"
      ],
      "metadata": {
        "id": "OItoTWqXBWTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(figsize=(9, 6))\n",
        "\n",
        "# Checking the box plot for age feature\n",
        "print(\"Annual_Income Shape:\",df.shape)\n",
        "## Max and Min Quantile\n",
        "max_val = df['Annual_Income'].quantile(0.75)\n",
        "min_val = df['Annual_Income'].quantile(0.25)\n",
        "\n",
        "sns.boxplot(df['Annual_Income'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oNeQ7Np0BeSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When checking the boxplot for **Annual_Income** feature, we can see that the values has started breaking from a point around 2x1e6. Therefore I will be removing the outliers after $incomes=2 \\times 1e6$"
      ],
      "metadata": {
        "id": "4iOZw6ugGxlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing datapoints that have income values greater than 2 x 1e6\n",
        "df = df[(df['Annual_Income']< 2e6)]\n",
        "\n",
        "print(\"After Annual_Income Shape:\",df.shape)\n",
        "\n",
        "sns.boxplot(df['Annual_Income'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l3dlq2NwHgob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Checking the **'Debt_To_Income'** feature for outliers"
      ],
      "metadata": {
        "id": "QE4174blEMuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(figsize=(9, 6))\n",
        "\n",
        "# Checking the box plot for age feature\n",
        "print(\"Debt_To_Income Shape:\",df.shape)\n",
        "## Max and Min Quantile\n",
        "max_val = df['Debt_To_Income'].quantile(0.75)\n",
        "min_val = df['Debt_To_Income'].quantile(0.25)\n",
        "\n",
        "sns.boxplot(df['Debt_To_Income'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K7VT9P2gEfCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no outliers in the **Debt_to_income** feature."
      ],
      "metadata": {
        "id": "SKRJlN8wIBV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Checking the **'Inquiries_Last_6Mo'** feature for outliers"
      ],
      "metadata": {
        "id": "EU-USXqVE00g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(figsize=(9, 6))\n",
        "\n",
        "# Checking the box plot for age feature\n",
        "print(\"Inquiries_Last_6Mo Shape:\",df.shape)\n",
        "## Max and Min Quantile\n",
        "max_val = df['Inquiries_Last_6Mo'].quantile(0.75)\n",
        "min_val = df['Inquiries_Last_6Mo'].quantile(0.25)\n",
        "\n",
        "sns.boxplot(df['Inquiries_Last_6Mo'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iqGz40SjE6bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When checking the boxplot for the **'Inquiries_Last_6Mo'** feature, we can see that there are no significant outliers, and that there are many datapoints that are outside the boxplot. Therefore, I will not be removing the datapoints that are identified here as outliers, since they can carry information in them."
      ],
      "metadata": {
        "id": "bZagOWqPIjRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Checking the **'Months_Since_Deliquency'** feature for outliers"
      ],
      "metadata": {
        "id": "nDv5HAhhFLhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(figsize=(9, 6))\n",
        "\n",
        "# Checking the box plot for age feature\n",
        "print(\"Months_Since_Deliquency Shape:\",df.shape)\n",
        "## Max and Min Quantile\n",
        "max_val = df['Months_Since_Deliquency'].quantile(0.75)\n",
        "min_val = df['Months_Since_Deliquency'].quantile(0.25)\n",
        "\n",
        "sns.boxplot(df['Months_Since_Deliquency'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bd8oC6P_FTm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When checking the boxplot for the **'Months_Since_Deliquency'** feature, we can see that there are no significant outliers, and that there are many datapoints that are outside the boxplot. Therefore, I will not be removing the datapoints that are identified here as outliers, since they can carry information in them."
      ],
      "metadata": {
        "id": "u26_0eT4JT9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Checking the **'Number_Open_Accounts'** feature for outliers  "
      ],
      "metadata": {
        "id": "TWOwjDjAFh0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(figsize=(9, 6))\n",
        "\n",
        "# Checking the box plot for age feature\n",
        "print(\"Number_Open_Accounts Shape:\",df.shape)\n",
        "## Max and Min Quantile\n",
        "max_val = df['Number_Open_Accounts'].quantile(0.75)\n",
        "min_val = df['Number_Open_Accounts'].quantile(0.25)\n",
        "\n",
        "sns.boxplot(df['Number_Open_Accounts'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1EUiXDqnFnKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When checking the above box plot, we can see that, there a significant gap has first occured around the number accounts value 58. Therefore I decided to clear the datapoints after number accounts 58 as handling outliers in this feature."
      ],
      "metadata": {
        "id": "dMmYzX2-Jlyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing datapoints that have number open accounts greater than 58\n",
        "df = df[(df['Number_Open_Accounts']<58)]\n",
        "\n",
        "print(\"After Number_Open_Accounts Shape:\",df.shape)\n",
        "\n",
        "sns.boxplot(df['Number_Open_Accounts'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IHKFimAUJ198"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Checking the **'Total_Accounts'** feature for outliers   "
      ],
      "metadata": {
        "id": "YqsFV4KjGErt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(figsize=(9, 6))\n",
        "\n",
        "# Checking the box plot for age feature\n",
        "print(\"Total_Accounts Shape:\",df.shape)\n",
        "## Max and Min Quantile\n",
        "max_val = df['Total_Accounts'].quantile(0.75)\n",
        "min_val = df['Total_Accounts'].quantile(0.25)\n",
        "\n",
        "sns.boxplot(df['Total_Accounts'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5XpOfyRSFyKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When checking the above box plot, we can see that, there a significant gap has first occured around the total accounts value 120. Therefore I decided to clear the datapoints after total accounts 120 as handling outliers in this feature"
      ],
      "metadata": {
        "id": "BNPbbrtwQf8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing datapoints that have total accounts greater than 120\n",
        "df = df[(df['Total_Accounts']<120)]\n",
        "\n",
        "print(\"After Total_Accounts shape:\",df.shape)\n",
        "\n",
        "sns.boxplot(df['Total_Accounts'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KD2ljO_RQxCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Checking the **'Loan_Amount_Requested'** feature for outliers"
      ],
      "metadata": {
        "id": "a8zj7OC0atnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(figsize=(9, 6))\n",
        "\n",
        "# Checking the box plot for age feature\n",
        "print(\"Loan_Amount_Requested Shape:\",df.shape)\n",
        "## Max and Min Quantile\n",
        "max_val = df['Loan_Amount_Requested'].quantile(0.75)\n",
        "min_val = df['Loan_Amount_Requested'].quantile(0.25)\n",
        "\n",
        "sns.boxplot(df['Loan_Amount_Requested'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wqHUfsv-aDrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing datapoints that have income values greater than 50\n",
        "df = df[(df['Loan_Amount_Requested']< 50)]\n",
        "\n",
        "print(\"After Loan_Amount_Requested Shape:\",df.shape)\n",
        "\n",
        "sns.boxplot(df['Loan_Amount_Requested'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tmTkppMPaN5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, after carefully inspection of all the numerical fields (features) in the dataset, I have removed outliers and the remaining number datapoints is 139186. Therefore I have removed $164309 - 139186 = 25123$ outliers."
      ],
      "metadata": {
        "id": "CQqSDXouSsIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Encoding**\n",
        "\n",
        "In this process, the categorical data are encoded into numerical data. The LabelEncoder is used to encode the class values to integers accordingly as follows.\n",
        "\n"
      ],
      "metadata": {
        "id": "pjWdG5aZTRUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "tpIued0gk2a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "8egFka-SnDU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "df['Length_Employed'].value_counts()"
      ],
      "metadata": {
        "id": "aZZILjTeTlj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Home_Owner'].value_counts()"
      ],
      "metadata": {
        "id": "EX6u9auUlW4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace NaN values with mean value\n",
        "df.Months_Since_Deliquency = df.Months_Since_Deliquency.fillna(df.Months_Since_Deliquency.median())"
      ],
      "metadata": {
        "id": "aKMqEHlUm1N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Length_Employed = df.Length_Employed.fillna('< 1 year')\n",
        "df.Home_Owner = df.Home_Owner.fillna('Other')"
      ],
      "metadata": {
        "id": "heEvuFe-np6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "lD2SYuI_onEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['Income_Verified', 'Purpose_Of_Loan','Length_Employed','Home_Owner']\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "for feature in categorical_features:\n",
        "    df[feature] = encoder.fit_transform(df[feature])"
      ],
      "metadata": {
        "id": "Ecv6RklMtHVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_valued_features = ['Gender']\n",
        "bin_dict = {'Female':0, 'Male':1}\n",
        "# Replace binary values in df using the provided dictionary\n",
        "for item in binary_valued_features:\n",
        "  df.replace({item:bin_dict},inplace=True)"
      ],
      "metadata": {
        "id": "m7mMMM-2qPVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Unwanted Feature**"
      ],
      "metadata": {
        "id": "yZZIuXVzqq4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Loan_ID'], inplace=True, axis = 1)\n",
        "df"
      ],
      "metadata": {
        "id": "bKCaKpuWqwhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this point, we have encoded all the values in the dataset into numerical values"
      ],
      "metadata": {
        "id": "b8dnmVHyrhPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "O-1D-bL1vE4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().transpose()"
      ],
      "metadata": {
        "id": "QtoqXpzDrztq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting Data**"
      ],
      "metadata": {
        "id": "FAJvBjPnrg2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['Interest_Rate']\n",
        "X = df.values[:, :-1] # get all columns except the last column\n",
        "\n",
        "# spliting training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=50)"
      ],
      "metadata": {
        "id": "l3GNcYqrt666"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here when using train_test_split, we use a random_state initializing value to make sure that the data splitting is done in the same way even in a different run of the code."
      ],
      "metadata": {
        "id": "zVllFW6LuICJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Scaling**\n",
        "\n",
        "After encoding categorical data, the dataset consists of features with different data ranges. These values are standardized and feature scaling is done as follows. Numerical features were scaled by removing the\n",
        "mean and by scaling to unit variance (StandardScaler) as follows."
      ],
      "metadata": {
        "id": "ZyeFq3p1uMGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "ohe-BcPduZqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)"
      ],
      "metadata": {
        "id": "G3NZMY6WvBHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Engineering**\n",
        "\n",
        "Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of our model. The data features that we use to train your machine learning models have a huge influence on the performance we can achieve. Irrelevant or partially relevant features can negatively impact model performance."
      ],
      "metadata": {
        "id": "58LhOSZfwW8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drawing the Correlation Matrix**\n",
        "\n",
        "Therefore I will be performing the Correlation Coefficient checking mechanism in order to check the relationship between the different features with the output.\n",
        "\n",
        "Each of those correlation types can exist in a spectrum represented by values from 0 to 1 where slightly or highly positive correlation features can be something like 0.5 or 0.7. If there is a strong and perfect positive correlation, then the result is represented by a correlation score value of 0.9 or 1."
      ],
      "metadata": {
        "id": "bgRhg2_CwrMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# draw the correlation matrix\n",
        "correlation_matrix = pd.DataFrame(X_train).corr()\n",
        "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
        "sns.heatmap(correlation_matrix, ax=ax)\n",
        "correlation_matrix"
      ],
      "metadata": {
        "id": "DeqmGGypw7qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After generating the correlation matrix, we can see that to the right side of the matrix, there are features that has a very high correlation. We usually remove such features that have high correlations because, they are some what linearly dependent with other features. These features contribute very less in predicting the output but increses the computational cost.\n",
        "\n",
        "It is clear that correlated features means that they bring the same information, so it is logical to remove one of them.\n",
        "\n",
        "In order to find the exact columns that has the high correlation values, I perform the below code. I am checking the upper triangle of the correlation matrix because the upper and lower traingles are mirrors of each other that are divided by the diagonal in the correlation matrix. Here I am checking the columns that has correlations values more than 0.6 with the hope of removing them."
      ],
      "metadata": {
        "id": "zEk6kv2txVhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the upper triangle of the correlation matrix\n",
        "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape),k=1).astype(np.bool))\n",
        "print(upper_tri)\n",
        "\n",
        "# checking which columns can be dropped\n",
        "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
        "print('\\nTo drop')\n",
        "print(to_drop)\n",
        "\n",
        "# removing the selected columns\n",
        "X_train = X_train.drop(X_train.columns[to_drop], axis=1)\n",
        "X_test = X_test.drop(X_test.columns[to_drop], axis=1)\n",
        "print(X_train.head())"
      ],
      "metadata": {
        "id": "6W0lmp9Kxwxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, after performing the above code, we can see that there are no columns that has more than 0.95 correlation and that therefore, there are no columns to be removed."
      ],
      "metadata": {
        "id": "fuK5JFbYA3w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Appyling PCA**\n",
        "\n",
        "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set."
      ],
      "metadata": {
        "id": "pHiTJyay0VI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# apply the PCA for feature for feature reduction\n",
        "pca = PCA(n_components=0.95)\n",
        "pca.fit(X_train)\n",
        "PCA_X_train = pca.transform(X_train)\n",
        "PCA_X_test = pca.transform(X_test)\n",
        "\n",
        "X_train"
      ],
      "metadata": {
        "id": "6dgn4Hic0LUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I have not manually set the n_components of the PCA model. We want the explained variance to be between 95–99%. Therefore, i have set the PCA's n_components to 0.95"
      ],
      "metadata": {
        "id": "NHVCrU8B_31u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Developing the MultiLayer Perceptron Model**"
      ],
      "metadata": {
        "id": "wXoLDxmY0tTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MLPRegressor**"
      ],
      "metadata": {
        "id": "0BuvqVgj9fYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mlp = MLPRegressor(activation='logistic')\n",
        "mlp.fit(PCA_X_train,y_train)"
      ],
      "metadata": {
        "id": "GmZfaXhW9exU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = mlp.predict(PCA_X_test)\n",
        "predictions1= mlp.predict(PCA_X_train)\n",
        "print(\"mse_test :\" ,mean_squared_error(y_test,predictions), \"mse_train :\",mean_squared_error(y_train,predictions1))"
      ],
      "metadata": {
        "id": "CfB6XhKS994S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MLPClassifier**"
      ],
      "metadata": {
        "id": "-kzpN5aXAaJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# define and train an MLPClassifier named mlp on the given data\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,200,50), max_iter=300, activation='relu', solver='adam', random_state=1)\n",
        "mlp.fit(PCA_X_train, y_train)"
      ],
      "metadata": {
        "id": "9eJrXQOA0vSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy')\n",
        "print(mlp.score(PCA_X_test, y_test))"
      ],
      "metadata": {
        "id": "tCPbX7Zg79mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict = mlp.predict(PCA_X_test)"
      ],
      "metadata": {
        "id": "9czRAWwi8Pen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find the Mean Squared Error (MSE) and other scores as follows."
      ],
      "metadata": {
        "id": "qYTD9_HL8MHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "# print the training error and MSE\n",
        "print(\"Training error: %f\" % mlp.loss_curve_[-1])\n",
        "print(\"Training set score: %f\" % mlp.score(PCA_X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(PCA_X_test, y_test))\n",
        "print(accuracy_score(y_test, predict))\n",
        "\n",
        "print(\"MSE: %f\" % mean_squared_error(y_test, predict))"
      ],
      "metadata": {
        "id": "L5epKPp88IlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
